{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VEnctD3ocWZU"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","from urllib.parse import urljoin"]},{"cell_type":"code","source":["# Base URL of the Human Rights Library\n","base_url = \"http://hrlibrary.umn.edu/instree/ainstls1.htm\"\n","general_base_url = \"http://hrlibrary.umn.edu\"\n","# File path for valid URLs\n","output_valid_urls = \"URL.txt\"\n","# List of languages and file types to exclude\n","excluded_languages = ['/arab/', '/spanish/', '/russian/', '/japanese/', '/K-', '/chinese/', '/Chinese/', '/french/', '/farsi/']\n","excluded_file_types = ['.pdf', '.doc']\n"],"metadata":{"id":"fi88z0IfcfQ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to check if a URL is valid\n","def is_url_valid(url):\n","    try:\n","        response = requests.head(url, allow_redirects=True, timeout=5)  # Use HEAD to check URL\n","        return response.status_code == 200  # Return True if the status code is 200\n","    except requests.RequestException:\n","        return False  # Return False if any request exception occurs\n","# Function to check if a URL should be excluded\n","def should_exclude_url(url):\n","    return any(lang in url for lang in excluded_languages) or any(url.endswith(ft) for ft in excluded_file_types)\n"],"metadata":{"id":"XpzeTTUTcfOg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to scrape links from the main page\n","def scrape_links():\n","    # Send a GET request to the main page\n","    response = requests.get(base_url)\n","    # Check if the request was successful\n","    if response.status_code == 200:\n","        # Parse the HTML content\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","        # Find all links to documents\n","        links = soup.find_all('a')\n","        # Extract and store the document links\n","        valid_urls = []\n","        for link in links:\n","            href = link.get('href')\n","            if href:\n","                full_url = urljoin(base_url, href)  # Convert relative URLs to absolute URLs\n","                # Check if the URL starts with the general base URL, is valid, and does not contain excluded languages or file types\n","                if full_url.startswith(general_base_url) and not should_exclude_url(full_url):\n","                    if is_url_valid(full_url):\n","                        valid_urls.append(full_url)\n","        # Remove duplicates by converting to a set and then back to a list\n","        unique_valid_urls = list(set(valid_urls))\n","        # Sort the URLs alphabetically (optional)\n","        unique_valid_urls.sort()\n","        # Save valid URLs to a text file\n","        with open(output_valid_urls, 'w') as valid_file:\n","            for url in unique_valid_urls:\n","                valid_file.write(url + '\\n')\n","        print(f\"Saved {len(unique_valid_urls)} unique valid URLs to {output_valid_urls}.\")\n","    else:\n","        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n"],"metadata":{"id":"hQQUsECucfMA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run the scraping function\n","if __name__ == \"__main__\":\n","    scrape_links()"],"metadata":{"id":"Q_A_-RYHcfJw"},"execution_count":null,"outputs":[]}]}